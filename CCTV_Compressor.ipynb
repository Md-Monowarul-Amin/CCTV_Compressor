{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WDzL7pH3egId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TXO577Xea1R"
      },
      "outputs": [],
      "source": [
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "\n",
        "class Cam2WorldMapper:\n",
        "    \"\"\"Maps points from image to world coordinates using perspective transform.\"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self.M: np.ndarray | None = None\n",
        "\n",
        "    def __call__(self, image_pts) -> np.ndarray :\n",
        "        return self.map(image_pts)\n",
        "\n",
        "    def find_perspective_transform(self, image_pts, world_pts) -> np.ndarray:\n",
        "        image_pts = np.asarray(image_pts, dtype=np.float32).reshape(-1, 1, 2)\n",
        "        world_pts = np.asarray(world_pts, dtype=np.float32).reshape(-1, 1, 2)\n",
        "        self.M = cv.getPerspectiveTransform(image_pts, world_pts)\n",
        "        return self.M\n",
        "\n",
        "    def map(self, image_pts) -> np.ndarray:\n",
        "        if self.M is None:\n",
        "            raise ValueError(\"Perspective transform not estimated\")\n",
        "        image_pts = np.asarray(image_pts, dtype=np.float32).reshape(-1, 1, 2)\n",
        "        return cv.perspectiveTransform(image_pts, self.M).reshape(-1, 2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import easyocr\n",
        "from cam2world_mapper import Cam2WorldMapper\n",
        "import supervision as sv\n",
        "import numpy as np\n",
        "import time\n",
        "import copy\n",
        "\n",
        "yolo_model = YOLO('yolo11n.pt')\n",
        "\n",
        "names = {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
        "\n",
        "\n",
        "\n",
        "# current_frame_vehicle_ids = set()\n",
        "# current_frame_number_plate_ids = set()\n",
        "\n",
        "# load video\n",
        "video_path = './entry_room.webm'\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "A, B, C, D = (198, 168), (479, 200), (558, 331), (72, 333)\n",
        "\n",
        "image_pts = [A, B, C, D]\n",
        "# M6 is roughly 32 meters wide and 140 meters long there.\n",
        "world_pts = [(0, 0), (2.5, 0), (3, 2), (0, 2.5)]\n",
        "\n",
        "saved_frames = []\n",
        "person_dict = {}\n",
        "\n",
        "mapper = Cam2WorldMapper()\n",
        "mapper.find_perspective_transform(image_pts, world_pts)\n",
        "\n",
        "\n",
        "######################## Testing on an Image ##############################\n",
        "\n",
        "# results = plate_detection_model.predict(\"./car_plate.jpeg\")\n",
        "# img_path = \"./car_plate2.jpeg\"\n",
        "# image = cv2.imread(img_path)\n",
        "# # cv2.imshow(\"OpenCV Image\",image)\n",
        "# # cv2.waitKey(0)\n",
        "# print(image)\n",
        "# results = plate_detection_model.predict(img_path)\n",
        "# out = results[0].plot()\n",
        "\n",
        "# plate = results[0].boxes.xyxy[0]\n",
        "# x, y, w, h = plate  # Get the bounding box coordinates\n",
        "# x, y, w, h = int(x), int(y), int(w), int(h)  # Convert to integers\n",
        "\n",
        "# # Extract the text (if any)\n",
        "# plate_text = perform_ocr_on_image(image, [x, y, w, h])\n",
        "\n",
        "#             # Draw the bounding box and the detected text on the frame (optional)\n",
        "# cv2.rectangle(image, (x, y), (w, h), (0, 255, 0), 2)\n",
        "# cv2.putText(image, plate_text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "# cv2.imshow('frame', image)\n",
        "# cv2.waitKey(0)\n",
        "\n",
        "# print(\"Detected Plate Text: \", plate_text)\n",
        "\n",
        "###########################////////////////////////##############################\n",
        "\n",
        "\n",
        "def generate_compressed_video(saved_frames):\n",
        "    output_video = \"selected_frames_video.mp4\"\n",
        "    frame_rate = 30  # Adjust as needed\n",
        "\n",
        "        # Read the first frame to get dimensions\n",
        "    first_frame = saved_frames[0]\n",
        "    height, width, _ = first_frame.shape\n",
        "\n",
        "        # Initialize VideoWriter\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")  # Codec\n",
        "    out = cv2.VideoWriter(output_video, fourcc, frame_rate, (width, height))\n",
        "\n",
        "        # Write selected frames to the video\n",
        "    for frame in saved_frames:\n",
        "        out.write(frame)\n",
        "\n",
        "\n",
        "def detect_entry(person_id : int, transformed_coordinates:list):\n",
        "    x_t, y_t, w_t, h_t = transformed_coordinates\n",
        "    if person_id not in person_dict:\n",
        "        person_dict[person_id] = dict()\n",
        "        person_dict[person_id][\"prev_x_t\"] = x_t\n",
        "        person_dict[person_id][\"x_t\"] = x_t\n",
        "        person_dict[person_id][\"last_track\"] = time.time()\n",
        "\n",
        "    else:\n",
        "        if \"x_t\" in person_dict[person_id]:\n",
        "            person_dict[person_id][\"prev_x_t\"] = person_dict[person_id][\"x_t\"]\n",
        "            person_dict[person_id][\"x_t\"] = x_t\n",
        "        else:\n",
        "            person_dict[person_id][\"x_t\"] = x_t\n",
        "\n",
        "\n",
        "    if abs(person_dict[person_id][\"last_track\"] - time.time()) >= 1:\n",
        "        if person_dict[person_id][\"x_t\"] < person_dict[person_id][\"prev_x_t\"] and x_t < .5:\n",
        "            person_dict[person_id][\"action\"] = \"enter\"\n",
        "\n",
        "        elif person_dict[person_id][\"x_t\"] > person_dict[person_id][\"prev_x_t\"] and x_t < .5:\n",
        "            person_dict[person_id][\"action\"] = \"out\"\n",
        "            print(\"OUT: \", person_dict[person_id][\"x_t\"], person_dict[person_id][\"prev_x_t\"])\n",
        "        person_dict[person_id][\"last_track\"] = time.time()\n",
        "\n",
        "\n",
        "\n",
        "ret = True\n",
        "# read frames\n",
        "while ret:\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if ret:\n",
        "\n",
        "        results = yolo_model.track(frame, persist=True)\n",
        "        # print(results[0].boxes)\n",
        "\n",
        "        for i in range(len(results[0].boxes)):        #### Working on each bounding box element\n",
        "            box_cls = results[0].boxes.cls[i].tolist()\n",
        "            if names[box_cls] == \"person\":\n",
        "                box_coordinates_video = results[0].boxes.xyxy[i].tolist()\n",
        "                x, y, w, h = box_coordinates_video\n",
        "                x, y, w, h = int(x), int(y), int(w), int(h)\n",
        "\n",
        "                x_t, y_t, w_t, h_t = mapper.map([x, y, w, h]).flatten()\n",
        "\n",
        "                person_id = results[0].boxes.id[i]\n",
        "                person_id = int(person_id)\n",
        "\n",
        "                detect_entry(person_id, [x_t, y_t, w_t, h_t])  ## Detect whether entry or out\n",
        "\n",
        "                # f.write( str(vehicles))\n",
        "                # print(vehicles)\n",
        "                # text = \"id: \" + str(int(car_id)) + \" x: \" + str(x) + \" y: \" + str(y) + \" speed: \" + str( cars[car_id][\"speed\"]) + \" km/h\"\n",
        "\n",
        "                entry_text = \" \"\n",
        "                if \"action\" in person_dict[person_id]:\n",
        "                    if person_dict[person_id][\"action\"] == \"enter\":\n",
        "                        entry_text = str(names[box_cls]) + \" \" + \"Entering \" + \" Room\"\n",
        "                    elif person_dict[person_id][\"action\"] == \"out\":\n",
        "                        entry_text = str(names[box_cls]) + \" \" + \"getting OUT of the\" + \" Room\"\n",
        "\n",
        "                # cv2.rectangle(frame, (220, 5), (330, 40), (0,255,0), 2)\n",
        "                cv2.putText(frame, entry_text, (120, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0, 255), 2)\n",
        "\n",
        "                cv2.rectangle(frame, (x, y), (w, h), (0,255,0), 2)\n",
        "                cv2.putText(frame, \"Person\" + str(x_t) + \" \", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,255,0), 2)\n",
        "\n",
        "                if x_t < 0:\n",
        "                    saved_frames.append(frame)\n",
        "\n",
        "\n",
        "                    # print(x, y, w, h)\n",
        "\n",
        "        # print(car_results[0])\n",
        "        cv2.imshow(\"cars:\", frame)\n",
        "\n",
        "\n",
        "####################### >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> #######################\n",
        "\n",
        "######################## Testing Bounding Regions for Speed Test ###################\n",
        "\n",
        "        # img = frame\n",
        "        # # cv2.resize(img, (120, 200))\n",
        "        # color1 = sv.Color.from_hex(\"#004080\")\n",
        "        # color2 = sv.Color.from_hex(\"#f78923\")\n",
        "        # poly = np.array(((198, 168), (479, 200), (558, 331), (72, 333)))  # A=1200, 700 B= 2800, 700 C = 3800, 2200 D= 501, 2200\n",
        "\n",
        "        # # poly = np.array(((240, 200), (900, 200), (900, 400), (-400, 500)))  # A=1200, 700 B= 2800, 700 C = 3800, 2200 D= 501, 2200\n",
        "\n",
        "\n",
        "        # img = sv.draw_filled_polygon(img, poly, color1, 0.5)\n",
        "        # img = sv.draw_polygon(img, poly, sv.Color.WHITE, 12)\n",
        "        # img = sv.draw_text(img, \"A\", sv.Point(800, 370), color2, 2, 6)\n",
        "        # img = sv.draw_text(img, \"B\", sv.Point(1125, 370), color2, 2, 6)  ## (100, 100), (1200, 100), (1200, 400), (-100, 400)\n",
        "        # img = sv.draw_text(img, \"C\", sv.Point(1880, 780), color2, 2, 6)\n",
        "        # img = sv.draw_text(img, \"D\", sv.Point(40, 780), color2, 2, 6)\n",
        "\n",
        "\n",
        "        # cv2.imshow(\"check: \", img)\n",
        "\n",
        "##################################>>>>>>>>>>################################\n",
        "\n",
        "\n",
        "\n",
        "        # visualize\n",
        "        # cv2.imshow('frame', results[0].plot())\n",
        "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "\n",
        "generate_compressed_video(saved_frames)"
      ],
      "metadata": {
        "id": "g1_tXDl_eoXG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}